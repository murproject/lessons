{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mur_lesson_coral_exmaple.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsH4ZlfiDg-4"
      },
      "source": [
        "Урок по использованию Google Coral AI Accelerator Module: тренируем модель для распознавания картинок подводным аппаратом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_YU5KhpEdfJ"
      },
      "source": [
        "# Обучаем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R73ZiCppyZjH"
      },
      "source": [
        "'''\n",
        "Перед тем как начать, нужно сначала загрузить датасет.\n",
        "Нижеуказанный архив содержит размеченные изображения,\n",
        "разбитые на три группы (train, validation, test).\n",
        "\n",
        "Загрузка и распаковка архива:\n",
        "'''\n",
        "\n",
        "!wget \"https://raw.githubusercontent.com/murproject/lessons/master/18.%20Objects%20detection%20with%20Google%20Coral/dataset.zip\"\n",
        "!unzip -o dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaJ8iqtatUBw"
      },
      "source": [
        "'''\n",
        "Установим необходимые библиотеки из репозитория PIP.\n",
        "'''\n",
        "\n",
        "!pip install -q tflite-model-maker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvqGQq2zt_Ak"
      },
      "source": [
        "'''\n",
        "Импортируем необходимые нам библиотеки.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v34SABSfuABF"
      },
      "source": [
        "'''\n",
        "Задаём датасет, состоящий из трёх групп. Указываем путь к распакованным данным,\n",
        "а затем заполняем массивы данных и выводим количество кадров в каждой группе.\n",
        "'''\n",
        "\n",
        "path = '/content/data/'\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "for group in ['train', 'validation', 'test']:\n",
        "    dataset[group] = object_detector.DataLoader.from_pascal_voc(\n",
        "        label_map = path + 'img_labels.yaml',\n",
        "        images_dir = path + group,\n",
        "        annotations_dir = path + group\n",
        "    )\n",
        "\n",
        "    print('size of %s:' % group , len(dataset[group]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxC4djZJXh4"
      },
      "source": [
        "'''\n",
        "Указываем спецификацию модели, которую будем использовать.\n",
        "Мы воспользуемся моделью из семейства EfficientDet:\n",
        "это как раз подходит под наши цели (распознавание объектов),\n",
        "а также совместимо с Edge TPU (то есть, способно работать\n",
        "на Coral, т.к. в его основе лежит именно эта технология).\n",
        "\n",
        "Данное семейство моделей включает несколько вариаций,\n",
        "отличающихся размером (от модели Lite0 размером 4.4 MB,\n",
        "до Lite4 размером 19.9 MB), при этом один модуль Coral\n",
        "не способен работать с моделями более 8 MB. Чем больше\n",
        "размер модели, тем выше точность, но и выше задержка\n",
        "распознавания. Мы воспользуемся EfficientDet-Lite2\n",
        "размером в 7.2 MB - самая объёмная, которая может быть\n",
        "запущена на одном модуле Coral.\n",
        "'''\n",
        "\n",
        "spec = object_detector.EfficientDetLite2Spec()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0kYpccavND5"
      },
      "source": [
        "'''\n",
        "Начинаем процесс обучения. Здесь мы указываем переменные с датасетом,\n",
        "ранее выбраную спецификацию модели, а также ещё важные параметры:\n",
        "количство эпох и размер одного этапа.\n",
        "\n",
        "В процессе обучения, происходит многократная тренировка нейронной сети\n",
        "на всём датасете. Один такой полный проход - эпоха. Если эпох слишком мало,\n",
        "то сеть может быть недостаточно обучена, а при слишком большом их количестве\n",
        "возможно переобучение (когда нейросеть, образно говоря, слишком \"хорошо запомнила\"\n",
        "обучающую выборку, и плохо срабатывает на данных, которые даже немного от\n",
        "неё отличаются).\n",
        "\n",
        "Обучение является довольно затратным процессом, и если попытаться провести\n",
        "тренировку на всём датасете в один проход, то можно столкнуться, к примеру,\n",
        "с исчерпанием оперативной памяти. Чтобы этого избежать, каждую эпоху можно разбить\n",
        "на несколько этапов, чтобы тренировка происходила небольшими порциями.\n",
        "\n",
        "Оптимальный подбор этих параметров может зависеть от конкретной используемой\n",
        "модели, а также от доступных ресурсов.\n",
        "\n",
        "Когда будет происходить процесс обучения, вы сможете наблюдать за ходом выполнения\n",
        "каждой эпохи: со временем значение ошибки становится всё меньше.\n",
        "'''\n",
        "\n",
        "model = object_detector.create(train_data=dataset['train'],\n",
        "                               model_spec=spec, \n",
        "                               validation_data=dataset['validation'], \n",
        "                               epochs=50, \n",
        "                               batch_size=20, # всего 380 кадров / 20 кадров на этап = 19 этапов\n",
        "                               train_whole_model=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj-zyEjXvQHV"
      },
      "source": [
        "'''\n",
        "После того, как модель была обучена, мы можем проверить качество распознавания\n",
        "на тестовой выборке. Мы можем получить примерный процент точности работы модели.\n",
        "'''\n",
        "\n",
        "model.evaluate(dataset['test'], batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1SelHSNvR-J"
      },
      "source": [
        "'''\n",
        "Модель обучена и её работа проверена, и теперь мы можем экспортировать её\n",
        "в файл. После этого, данную модель уже можно использовать на CPU и GPU\n",
        "при помощи библиотеки Tensorflow Lite, но на данный момент она ещё непригодна\n",
        "для использования на Google Coral.\n",
        "'''\n",
        "\n",
        "model.export(export_dir='.',\n",
        "             tflite_filename='rsub_model.tflite',\n",
        "             label_filename='rsub_labels.txt',\n",
        "             export_format=[ExportFormat.TFLITE, ExportFormat.LABEL])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls16nAhFEVga"
      },
      "source": [
        "# Экспортируем модель для Google Coral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nxu_oY4v8gA"
      },
      "source": [
        "'''\n",
        "Архитектура модуля Google Coral довольно специфична. Для того, чтобы модель\n",
        "могла работать на данном оборудовании, необходимо выполнить её преобразование\n",
        "в формат, совместимый с Edge TPU.\n",
        "\n",
        "Для начала установим необходимый для этого софт.\n",
        "'''\n",
        "\n",
        "\n",
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "! sudo apt-get update\n",
        "\n",
        "! sudo apt-get install edgetpu-compiler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovIdc0Qov-0j"
      },
      "source": [
        "'''\n",
        "После установки, можно выполнить преобразование модели.\n",
        "Выполнение модели может быть распределено на несколько модулей,\n",
        "но он у нас всего один, что и укажем в соответствующей переменной.\n",
        "'''\n",
        "\n",
        "!edgetpu_compiler rsub_model.tflite --num_segments=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REg2d6l0wCwL"
      },
      "source": [
        "'''\n",
        "Всё готово. Теперь мы можем загрузить все результаты нашей работы.\n",
        "'''\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('rsub_model.tflite') # модель для запуска с использованием Tensorflow Lite\n",
        "files.download('rsub_model_edgetpu.tflite') # модель для Edge TPU (Coral)\n",
        "files.download('rsub_labels.txt') # список категорий (классы объектов)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BugrNadWV1Ac"
      },
      "source": [
        "# Демонстрация работы\n",
        "\n",
        "Мы можем запустить обученную модель и без модуля Coral, воспользовавшись моделью, которая ещё не была преобразована для Edge TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCLte_Q8Xgu_"
      },
      "source": [
        "! python3 -m pip install --extra-index-url https://google-coral.github.io/py-repo/ pycoral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4740rGJ1XiWb"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import tflite_runtime.interpreter as tflite \n",
        "from pycoral.adapters import common\n",
        "from pycoral.adapters import detect\n",
        "from pycoral.utils.dataset import read_label_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9JJsYPxYC__"
      },
      "source": [
        "path_model = 'rsub_model.tflite'\n",
        "path_labels = 'rsub_labels.txt'\n",
        "\n",
        "threshold = 0.3\n",
        "max_detections = 4\n",
        "\n",
        "interpreter = tflite.Interpreter(path_model)\n",
        "interpreter.allocate_tensors()\n",
        "labels = read_label_file(path_labels)\n",
        "inference_size = common.input_size(interpreter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPES82agYVOD"
      },
      "source": [
        "def process_img(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img, inference_size)\n",
        "\n",
        "    common.set_input(interpreter, cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    interpreter.invoke()\n",
        "    objs = detect.get_objects(interpreter, threshold)[:max_detections]\n",
        "\n",
        "    drawing = img.copy()\n",
        "\n",
        "    height, width, channels = img.shape\n",
        "    scale_x, scale_y = width / inference_size[0], height / inference_size[1]\n",
        "\n",
        "    print(path, \"objects:\")\n",
        "    for obj in objs:\n",
        "        box = obj.bbox.scale(scale_x, scale_y)\n",
        "        p0 = (int(box.xmin), int(box.ymin))\n",
        "        p1 = (int(box.xmax), int(box.ymax))\n",
        "\n",
        "        percent = int(100 * obj.score)\n",
        "        label = '{}% {}'.format(percent, labels[obj.id])\n",
        "        print(label)\n",
        "        font = cv2.FONT_HERSHEY_DUPLEX\n",
        "\n",
        "        cv2.rectangle(drawing, p0, p1, (0, 255, 0), 1)\n",
        "        cv2.putText(drawing, label, (p0[0], p0[1] + 20), font, 0.7, (255, 255, 255), 1)\n",
        "\n",
        "    cv2_imshow(drawing)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw558oXhZ0cJ"
      },
      "source": [
        "dir = \"/content/data/test/\"\n",
        "\n",
        "for f in os.listdir(dir):\n",
        "    if f.endswith(\".jpg\"):\n",
        "      process_img(os.path.join(dir, f))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}